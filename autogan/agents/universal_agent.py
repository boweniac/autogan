import re
from collections import defaultdict
from typing import Optional, Dict, Any
from autogan.agents.agent_switch import AgentSwitch
from autogan.utils.compressed_messages_utils import compressed_messages
from autogan.utils.compressed_text_utils import compressed_text_universal
from autogan.oai.config_utils import AgentConfig
from autogan.oai.count_tokens_utils import count_text_tokens
from autogan.oai.generate_utils import generate_chat_completion
from autogan.utils.environment_utils import environment_info
from autogan.utils.response import default_response_func

try:
    from termcolor import colored
except ImportError:
    def colored(x, *args, **kwargs):
        return x


class UniversalAgent:
    def __init__(
            self,
            name: str,
            agent_config: Optional[Dict] = None,
            duty: Optional[str] = None,
            work_flow: Optional[str] = None,
            use_tool: Optional[str] = None,  # only | join
            super_rich: Optional[str] = None, # auto | on | off
            stream_mode: Optional[bool] = None,
    ):
        """Agent base class

        Each agent can communicate with other agents in the current department and the leader of the subordinate department to complete tasks together.
        每个 agent 可与当前部门的其他 agent 以及下级部门的 leader 沟通，协作完成任务。

        To provide functions beyond the modeling capabilities for the agent, you can override the tool_function method.
        想要为 agent 提供模型能力之外的功能，可以通过重写 tool_function 方法来实现。

        :param name: The agent name should be unique in the organizational structure.
            agent name 在组织架构中应当是唯一的。
        :param agent_config: The agent configuration includes:
            agent 配置包括：
            - main_model: The LLM configuration of the agent's main body.
                agent 主体的 LLM 配置。
            - summary_model: The LLM configuration used for compressing context and generating text summaries.
                用于压缩上下文以及生成文本摘要的 LLM 配置。
            - request_interval_time: The interval time of LLM requests.
                LLM 请求间隔时间。
            - request_timeout:The timeout of LLM requests.
                LLM 请求超时时间。
            - max_retries: The maximum number of retries for LLM requests.
                LLM 请求最大重试次数。
        :param duty: Used to explain one's job responsibilities to other agents.
            用于向其他 agent 说明自己的工作职责。
        :param work_flow: Defines the workflow of the agent.
            定义 agent 的工作流程。
        :param use_tool: Defines the mode of the agent using the tool_function:
            定义 agent 使用 tool_function 的模式：
            - None: means not using the tool function.
                不使用工具函数。
            - only: Do not use the LLM, only use the tool function to generate results.
                不使用 LLM，仅使用工具函数生成结果。
            - join: The content generated by the LLM will be used as the input parameter for the tool_function.
                LLM 生成的内容将作为 tool_function 的输入参数
        :param super_rich: Whether to enable the deep thought function. When enabled,
            it uses a set of analysis processes to refine the output of the agent. However,
            this can increase the number of tokens used, so it is not recommended for use with the gpt-4 model.
            The name "super_rich" is a reminder that using this function with gpt-4 can be expensive,
            even more so than Elon Musk's earning speed.
            是否开启深思功能，开启后会使用一套分析流程来收敛 agent 的输出结果，但这样做会增加 tokens 的消耗，因此不建议在gpt-4模型下使用。
            之所以这个参数叫 super_rich ，是为了提醒用户，如果在 gpt-4 下使用，其花钱的速度可能会超过马斯克赚钱的速度。
            - auto: Disable for GPT-4, enable for other models
                在 gpt-4下禁用，其他模型开启
            - on: Always enabled
                始终开启
            - off: Always disabled
                始终关闭
        :param stream_mode: Whether to enable the stream_mode
            定义 agent 的工作流程。
        """
        self.name = name
        self.agent_config = AgentConfig(agent_config) if agent_config else None
        self.duty = duty
        self.super_rich = super_rich # auto | on | off
        self.stream_mode = stream_mode
        self.response_func = default_response_func # Used to return results to the interface or terminal.
        self.workmates = "" # relevant personnel's name and duty
        self.pipeline = "" # In a linear workflow, this is the next person to communicate with.
        # Translate the session ID of the pusher into the sub-session ID of the receiver.
        self.sub_to_main_task_id = defaultdict(str)
        # Translate the session id of the sender into the superior session id of the receiver.
        self.main_to_sub_task_id = defaultdict(str)
        self._work_flow = work_flow
        self._use_tool = use_tool  # only | join
        self._conversation_messages = defaultdict(list)  # key: task id，value: Conversation history
        self._conversation_focus = defaultdict(Dict)  # key: task id，value: {"task_issuer": "", "task_content": ""}

    def set_agent_config(self, agent_config: Dict):
        self.agent_config = AgentConfig(agent_config)

    def new_task(self, switch: AgentSwitch, task_id: str, sender_name: str, content: str,
                 completion_tokens: int):
        """Accept tasks posted by other agent.

        :param switch: AgentSwitch object
        :param task_id: New task id
        :param sender_name: Task Issuer's Name
        :param content: Task content
        :param completion_tokens: Task content tokens
        """
        # Avoid excessively long task content
        if (self._use_tool != "only" and completion_tokens >
                self.agent_config.main_model_config.max_messages_tokens * 0.5):
            self._push_to_switch(switch, task_id, "The task is too long", 5)

        # Cache task information to maintain focus during task execution
        task_content = content.replace(f"@{self.name}", "please help me")
        task_content = task_content.replace(f"{switch.task_tag}", "")
        self._conversation_focus[task_id] = {'task_issuer': sender_name, 'task_content': task_content}
        # Start the generation process
        self._generate_process(switch, task_id, sender_name, content, completion_tokens)

    def receive(self, switch: AgentSwitch, task_id: str, sender_name: str, content: str,
                completion_tokens: int):
        """Receive messages sent by other agents (excluding new task requests)

        :param switch: AgentSwitch object
        :param task_id: Task id
        :param sender_name: Name of the agent sending the message
        :param content: Message content
        :param completion_tokens: Message content tokens
        """
        if self._use_tool != "only":
            safe_size = self.agent_config.main_model_config.max_messages_tokens
            if completion_tokens > safe_size:
                # 如消息内容过长，则对其进行压缩
                compressed_text, total_tokens = compressed_text_universal(
                    content, self.agent_config.summary_model_config,
                    self.name, self.response_func, self.stream_mode,
                    self._conversation_focus[task_id]['task_content'], safe_size)
                if compressed_text:
                    content = compressed_text
                    completion_tokens = total_tokens

            # Press the message into the session record of the current task
            self._conversation_messages[task_id].append(
                {'role': 'user', 'content': content, 'tokens': completion_tokens})

        # Start the generation process
        self._generate_process(switch, task_id, sender_name, content, completion_tokens)

    def tool_function(self, task_id: str, param: Optional[str] = None,
                      tokens: Optional[int] = None) -> tuple[str, int]:
        """When the value of the use_tool parameter is 'only' or 'join', please override this method.

        :return: --content: Generate content
            --tokens: Generate content tokens
        """
        pass

    def _base_message(self, switch: AgentSwitch, task_id: str) \
            -> tuple[dict[str, str], Optional[dict[str, Any]], int]:
        """This is the paradigm message required for each round of dialogue.
        每轮对话都需要的范式消息

        :param switch: AgentSwitch object
        :param task_id: Task id

        :return:
            -- system_message: Used to clarify its own workflow to the agent and where the agent can seek help.
                用于向 agent 阐明自身工作流程，以及可以向哪些 agent 寻求帮助。
            -- focus_message: Used to maintain focus during task execution, including who is currently executing the task and what the content of the task is. It will not be forgotten or compressed with the increase of dialogue rounds.
                用于在任务执行过程中保持专注力，包括当前正在执行谁发布的任务、任务的内容是什么。不会随会话轮次的增多而被遗忘或压缩。
            -- total_tokens: The overall tokens of the content of the system_message and the focus_message.
                system_message 以及 focus_message 内容的整体 tokens。
        """
        total_tokens = 0

        info = environment_info()

        # Assemble system message
        system_prompt = f"""Now your name is {self.name}, you are an assistant who will not give up easily when you encounter difficulties

Environment information:
{info}"""

        if self._work_flow:
            system_prompt += f"""

Your work flow is::
{self._work_flow}"""

        if self.workmates:
            system_prompt += f"""

The following professionals can help you accomplish the task:
{self.workmates}"""

        if self._use_tool is None:
            system_prompt += f"""
    
    Please follow these guidelines when replying to any content:
    1. Be aware that if you do not @recipient at the beginning, the system will give an error.
    2. When asking for help, you need to first post a task, the method is: @recipient {switch.task_tag} task content.
    3. The recipient does not have any dialogue records before the task begins, nor can they see your conversations with others.
    4. Do not suggest the recipient to communicate with others.
    5. Do not explain to the initiator of the task what you are going to do.
    6. In the reply, do not converse with two recipients at the same time.
            """

        total_tokens += 37

        system_message = {'role': 'system', 'content': system_prompt}
        if task_id in self._conversation_focus and self._conversation_focus[task_id]:
            # Assemble focus message
            focus_prompt = f"""current task content:
task issuer: {self._conversation_focus[task_id]['task_issuer']}
task content: {self._conversation_focus[task_id]['task_content']}"""

            if self._use_tool is None:
                if self.pipeline and self.pipeline != "\\":
                    focus_prompt += f"""

When you have the result of the task, please @{self.pipeline} {switch.task_tag} and reply to the execution result, He'll know what to do next"""
                else:
                    focus_prompt += f"""

When you have the result of the task, please @{self._conversation_focus[task_id]['task_issuer']} and reply to the execution result"""

            total_tokens += count_text_tokens(focus_prompt)

            focus_message = {'role': 'user', 'content': focus_prompt}
        else:
            focus_message = None

        return system_message, focus_message, total_tokens

    def _super_rich_message(self, switch: AgentSwitch, task_id: str, ideas: dict, index: int)\
            -> tuple[list[str, dict], bool]:
        """Thought prompts, with new content requested at each level
        深思提示词，每层请求新的内容

        :param switch: AgentSwitch object
        :param task_id: Task id
        :param ideas: Results generated
        :param index: Current thinking depth

        :return:
            -- message_list: Thought prompts list
                -- tag:
                -- message: Thought prompts
            -- is_end:
        """
        messages = []

        task_issuer = ""
        if self.pipeline and self.pipeline != "\\":
            task_issuer += f"{self.pipeline} : When there is no more work to be done, Submit the results to me."
        else:
            task_issuer += f"{self._conversation_focus[task_id]['task_issuer']} : When there is no more work to be done, Submit the results to me."

        total_tokens = 0

        info = f"""

reference workflow:
{environment_info()}"""

        workmates = ""
        if self.workmates:
            workmates = f"""

relevant personnel's name and duty:
{self.workmates}
{task_issuer}"""

        workflow = ""
        if self._work_flow:
            workflow = f"""
{self._work_flow}"""

        repetitive_prompt = f"""The above is a group chat record,  assuming you are {self.name}, please do the following analysis:

Step 1: Understand your overall workflow (No need to output):
    workflow:{workflow}

Step 2: Analyze whether {self.name} is repeating a task in the workflow or encountering difficulties (No need to output).

Step 3: output your analysis results
    If yes, please give advice on how to stop repeating from the perspective of {self.name}.
    If not, please reply one word 'None'."""

        messages.append(["Observe whether the previous conversation fell into a cycle", {'role': 'system', 'content': repetitive_prompt}])

        debug_prompt = f"""The above is a group chat record, please do the following analysis:

Step 1: Understand your overall workflow, Including the execution conditions and objectives for each step (No need to output):
    workflow:{workflow}
    
Step 2: Analyze whether there are unresolved errors in the previous conversation (No need to output).

Step 3: Analyze If there are unresolved errors, Think about what the root cause of these errors is (No need to output).

Step 4: Analyze If there are unresolved errors, From {self.name}'s perspective, how should you solve it next?  (No need to output)

Step 5: output your analysis results, including the following content:
    whether there are unresolved errors in the previous conversation:
        If there are unresolved errors, What errors in the dialogue:
        If there are unresolved errors, The root cause of the error:
        If there are unresolved errors, How to solve it next:

Note: There's no need to output the specific dialogue content, just output the analysis results."""

        messages.append(["Reflect on whether there are any errors in the previous dialogue process", {'role': 'system', 'content': debug_prompt}])

        planning_prompt = f"""The above is a group chat record, assuming you are {self.name}, please do the following analysis:

Step 1: Understand your overall workflow (No need to output):
    workflow:{workflow}

Step 2: Analyze which item to execute or continue to execute in the workflow (No need to output).

Step 3: Understand the specific errors that have occurred in the current conversation (No need to output).
    Are you stuck in a deadlock: {ideas["Observe whether the previous conversation fell into a cycle"]}
    
    {ideas["Reflect on whether there are any errors in the previous dialogue process"]}

Step 4: Understand some rules (No need to output).
    1. When asking for help, you need to first post a task,
    2. The recipient does not have any dialogue records before the task begins, nor can they see your conversations with others.
    2. Don't let the other party to communicate with others.
    3. In your plan, there should be no content about apologizing to others or what you are going to do.

Step 5: output your analysis results, including the following content:
    Do you need to create a task:
    In the next round of conversation, the specific work you need to do is(Please explain in detail and Ignore the work that has been completed.):
    all the details that need to be taken into consideration, including recommended methods or tools, etc:

Note: There's no need to output the specific dialogue content, just output the analysis results.
"""

        messages.append(["Think about what to do next", {'role': 'system', 'content': planning_prompt}])

        communicate_prompt = f"""your name is {self.name}, please do the following analysis:
        
Step 1: Understand your work plan (No need to output):
    {ideas["Think about what to do next"]}

Step 2: Get to know your colleagues, including what they can and cannot do (No need to output):
    {workmates}
    {self._conversation_focus[task_id]['task_issuer']} : ""
    
Step 3: Analyze who is the most relevant colleague to the first step of next round of conversation the specific work you need to do, note that you can only choose one person (No need to output).

Step 4: output your analysis results, including the following content:
    who is the most relevant colleague to the first step of your plan:
    What are the requirements when the other party receives messages:
    What can the other party do:
    What the other party cannot do:
    
Note: please provide the correct names of relevant personnel, Don't provide names that don't exist."""

        messages.append(["Think about who to communicate with next", {'role': 'user', 'content': communicate_prompt}])

        reply_prompt = f"""The above is a group chat record, assuming you are {self.name}, Please strictly follow the contents of the guidelines below to generate your response, note do not communicate with others or perform other tasks:

{info}

Step 1: Clarify who you will be communicating with (No need to output):
    {ideas["Think about who to communicate with next"]}

Step 2: Specify the task you are going to carry out (No need to output):
    {ideas["Think about what to do next"]}

Step 3: Understand some response rules (No need to output).
    1. Please do not mention the second person in your reply content.
    2. When you need to post a task, the method is: @recipient {switch.task_tag} task content.

Step 4: Please follow the content of the previous step, From {self.name}'s perspective, Output your response in the format below:
    @who you will be communicating with + Reply content"""

        messages.append(["Generate reply content", {'role': 'system', 'content': reply_prompt}])

        if index == len(messages) - 1:
            return messages[index], True
        else:
            return messages[index], False

    def _generate_process(self, switch: AgentSwitch, task_id: str, sender_name: str, content: str,
                          completion_tokens: int):
        """Generate process

        If the value of the use_tool parameter is None, only the main LLM is used to generate a response.
        如果 use_tool 参数的值为 None，则仅使用主体 LLM 生成回复。

        If the value of the use_tool parameter is 'only', the main LLM is skipped and the tool_function is used directly to generate a response.
        如果 use_tool 参数的值为 only，则跳过主体 LLM 直接使用 tool_function 生成回复。

        If the value of the use_tool parameter is 'join', the main LLM is first used to generate content, and then the generated content is used as the input parameter for tool_function.
        如果 use_tool 参数的值为 join，则先使用主体 LLM 生成内容，然后将生成的内容作为 tool_function 的输入参数。
        """
        hold_content = content
        hold_completion_tokens = completion_tokens
        try:
            if self._use_tool != "only":
                if self._use_tool == "join":
                    print(
                        colored(
                            f"\n\n>>>>>>>> tool call:",
                            "cyan",
                        ),
                        flush=True,
                    )
                    content, completion_tokens = self._base_generate_reply(switch, task_id, "tool_call")
                else:
                    if self.super_rich == "on":
                        content, completion_tokens = self._super_rich_generate_reply(switch, task_id)
                    elif (self.super_rich == "auto" or self.super_rich is None) and "gpt-4" not in self.agent_config.main_model_config.model:
                        content, completion_tokens = self._super_rich_generate_reply(switch, task_id)
                    else:
                        content, completion_tokens = self._base_generate_reply(switch, task_id, "main")
                if content is None:
                    raise ValueError("Failed to generate content.")
            else:
                content = re.sub(r'^@\S+\s+', '', content).strip()

            if self._use_tool and not content.startswith("@"):
                content, completion_tokens = self.tool_function(task_id, content, completion_tokens)
                # Assign recipients for the results generated by the tool_function.
                if not content.startswith("@"):
                    if (task_id in self._conversation_focus and "task_issuer" in
                            self._conversation_focus[task_id]):
                        receiver = self._conversation_focus[task_id]['task_issuer']
                    else:
                        receiver = sender_name
                    content = f"@{receiver} " + content
                self.response_func(self.name, "tool", "", False, 0, content, completion_tokens, None)
            self._push_to_switch(switch, task_id, content, completion_tokens)
        except SystemExit:
            print("The task is finished.")
        except Exception as e:
            print(f"e :{e}")
            if self._use_tool == "only":
                self._push_to_switch(switch, task_id, f"@{sender_name} Generate error, Trying again", 4)
            else:
                self._re_push_to_switch(switch, task_id, hold_content, hold_completion_tokens,
                                        sender_name)

    def _base_generate_reply(self, switch: AgentSwitch, task_id: str, gen: str) -> tuple[Optional[str], Optional[int]]:
        """Use the main LLM to generate responses.

        Before generating a response, the historical conversation records within the current task scope, excluding system_message and focus_message, will be compressed first.

        :param switch: AgentSwitch Object
        :param task_id: Task id

        :return: --content: Generate content
            --tokens: Generate content tokens
        """
        system_message, focus_message, total_tokens = self._base_message(switch, task_id)

        # Calculate the target size of context compression.
        safe_size = self.agent_config.main_model_config.max_messages_tokens - total_tokens
        # Compress the historical conversation records.
        request_messages, total_tokens = self._chat_messages_safe_size(task_id, safe_size)
        request_messages.insert(0, system_message)
        if focus_message:
            request_messages.insert(0, focus_message)
        return generate_chat_completion(self.agent_config.main_model_config, request_messages, self.name, gen, self.response_func, self.stream_mode)

    def _super_rich_generate_reply(self, switch: AgentSwitch, task_id: str) -> tuple[Optional[str], Optional[int]]:
        """Use the main LLM to generate responses.

        Before generating a response, the historical conversation records within the current task scope, excluding system_message and focus_message, will be compressed first.

        :param switch: AgentSwitch Object
        :param task_id: Task id

        :return: --content: Generate content
            --tokens: Generate content tokens
        """
        system_message, focus_message, total_tokens = self._base_message(switch, task_id)

        # Calculate the target size of context compression.
        safe_size = self.agent_config.main_model_config.max_messages_tokens - total_tokens

        # Compress the historical conversation records.
        request_messages, total_tokens = self._chat_messages_safe_size(task_id, safe_size)

        if focus_message:
            request_messages.insert(0, focus_message)

        index = 0
        ideas = defaultdict(str)
        while True:
            message, is_end = self._super_rich_message(switch, task_id, ideas, index)
            if is_end:
                gen = "main"
            else:
                gen = "idea"

            print(
                colored(
                    f"\n\n>>>>>>>> {message[0]}:",
                    "cyan",
                ),
                flush=True,
            )

            if message[1]["role"] == "system":
                messages = request_messages.copy()
                messages.append(message[1])
                content, token = generate_chat_completion(self.agent_config.main_model_config, messages, self.name, gen, self.response_func, self.stream_mode)
                ideas[message[0]] = content
                tokens = token
            else:
                content, token = generate_chat_completion(self.agent_config.main_model_config, [message[1]], self.name, gen, self.response_func, self.stream_mode)
                ideas[message[0]] = content
                tokens = token
            if is_end:
                break
            else:
                index += 1

        return content, tokens

    def _push_to_switch(self, switch: AgentSwitch, task_id: str, content: str, completion_tokens: int):
        content = content.replace(f"@{self.name} ", "")
        self._conversation_messages[task_id].append(
            {'role': 'assistant', 'content': content, 'tokens': completion_tokens})

        switch.handle_and_forward(task_id, self.name, content, completion_tokens)

    def _chat_messages_safe_size(self, task_id: str, safe_size: int) \
            -> tuple[list, int]:
        """Compress the historical session records within the current task scope (excluding system_message and focus_message)

        :param task_id: Task id
        :param safe_size: The max_messages_tokens of the main LLM configuration

        :return: --request_messages: It is used for the message content requested to LLM, with the tokens field of each message removed.
            –-total_tokens: The overall tokens after compression.
        """
        if task_id in self._conversation_messages and self._conversation_messages[task_id]:
            conversation_messages, request_messages, total_tokens = compressed_messages(
                self._conversation_messages[task_id], self._conversation_focus[task_id]['task_content'],
                self.agent_config.summary_model_config, self.name, self.response_func, self.stream_mode,
                safe_size)

            if request_messages:
                self._conversation_messages[task_id] = conversation_messages
                return request_messages, total_tokens

        return [], 0

    @staticmethod
    def _re_push_to_switch(switch: AgentSwitch, task_id: str, content: str, completion_tokens: int, sender: str):
        switch.handle_and_forward(task_id, sender, content, completion_tokens)